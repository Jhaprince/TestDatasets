 abstract we introduce a game-theoretic model to investigate the strategic interaction between a cyber insurance policyholder whose premium depends on her self-reported security level and an insurer with the power to audit the security level upon receiving an indemnity claim. audits can reveal fraudulent (or simply careless) policyholders not following reported security procedures, in which case the insurer can refuse to indemnify the policyholder. however, the insurer has to bear an audit cost even when the policyholders have followed the prescribed security procedures. as audits can be expensive, a key problem insurers face is to devise an auditing strategy to deter policyholders from misrepresenting their security levels to gain a premium discount. this decision-making problem was motivated by conducting interviews with underwriters and reviewing regulatory filings in the us; we discovered that premiums are determined by security posture, yet this is often self-reported and insurers are concerned by whether security procedures are practised as reported by the policyholders. to address this problem, we model this interaction as a bayesian game of incomplete information and devise optimal auditing strategies for the insurers considering the possibility that the policyholder may misrepresent her security level. to the best of our knowledge, this work is the first theoretical consideration of post-incident claims management in cyber security. our model captures the trade-off between the incentive to exaggerate security posture during the application process and the possibility of punishment for non-compliance with reported security policies. simulations demonstrate that common sense techniques are not as efficient at providing effective cyber insurance audit decisions as the ones computed using game theory.
 current techniques for explainable ai have been applied with some success to image processing. the recent rise of research in video processing has called for similar work n deconstructing and explaining spatio-temporal models. while many techniques are designed for 2d convolutional models, others are inherently applicable to any input domain. one such body of work, deep taylor decomposition, propagates relevance from the model output distributively onto its input and thus is not restricted to image processing models. however, by exploiting a simple technique that removes motion information, we show that it is not the case that this technique is effective as-is for representing relevance in non-image tasks. we instead propose a discriminative method that produces a naive representation of both the spatial and temporal relevance of a frame as two separate objects. this new discriminative relevance model exposes relevance in the frame attributed to motion, that was previously ambiguous in the original explanation. we observe the effectiveness of this technique on a range of samples from the ucf-101 action recognition dataset, two of which are demonstrated in this paper.
 in order to mimic the human ability of continual acquisition and transfer of knowledge across various tasks, a learning system needs the capability for continual learning, effectively utilizing the previously acquired skills. as such, the key challenge is to transfer and generalize the knowledge learned from one task to other tasks, avoiding forgetting and interference of previous knowledge and improving the overall performance. in this paper, within the continual learning paradigm, we introduce a method that effectively forgets the less useful data samples continuously and allows beneficial information to be kept for training of the subsequent tasks, in an online manner. the method uses statistical leverage score information to measure the importance of the data samples in every task and adopts frequent directions approach to enable a continual or life-long learning property. this effectively maintains a constant training size across all tasks. we first provide mathematical intuition for the method and then demonstrate its effectiveness in avoiding catastrophic forgetting and computational efficiency on continual learning of classification tasks when compared with the existing state-of-the-art techniques.



Rouge score with annotator1 : 
[{'rouge-1': {'f': 0.08994755917864329, 'p': 0.8187702265372169, 'r': 0.047587698673939624}, 'rouge-2': {'f': 0.0304027014256237, 'p': 0.2771474878444084, 'r': 0.01608352144469526}, 'rouge-l': {'f': 0.14226738464603533, 'p': 0.7, 'r': 0.07917992223400495}}]


Rouge score with annotator2 : 
[{'rouge-1': {'f': 0.08994755917864329, 'p': 0.8187702265372169, 'r': 0.047587698673939624}, 'rouge-2': {'f': 0.0304027014256237, 'p': 0.2771474878444084, 'r': 0.01608352144469526}, 'rouge-l': {'f': 0.14226738464603533, 'p': 0.7, 'r': 0.07917992223400495}}]


Rouge score with annotator3 : 
[{'rouge-1': {'f': 0.08994755917864329, 'p': 0.8187702265372169, 'r': 0.047587698673939624}, 'rouge-2': {'f': 0.0304027014256237, 'p': 0.2771474878444084, 'r': 0.01608352144469526}, 'rouge-l': {'f': 0.14226738464603533, 'p': 0.7, 'r': 0.07917992223400495}}]


average Rouge score : 
{'rouge-l': {'f': 0.14226738464603533, 'r': 0.07917992223400495, 'p': 0.6999999999999998}, 'rouge-2': {'f': 0.030402701425623697, 'r': 0.01608352144469526, 'p': 0.2771474878444084}, 'rouge-1': {'f': 0.08994755917864329, 'r': 0.04758769867393962, 'p': 0.8187702265372169}}


anti-redundance objective value :	nan
Average tf-idf objective value :	9.078184