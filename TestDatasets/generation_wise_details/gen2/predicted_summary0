 this paper presents a new measure of semantic similarity in an is-a taxonomy, based on the notion of information content. experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66).
 video action recognition, which is topical in computer vision and video analysis, aims to allocate a short video clip to a pre-defined category such as brushing hair or climbing stairs. recent works focus on action recognition with deep neural networks that achieve state-of-the-art results in need of high-performance platforms. despite the fast development of mobile computing, video action recognition on mobile devices has not been fully discussed. in this paper, we focus on the novel mobile video action recognition task, where only the computational capabilities of mobile devices are accessible. instead of raw videos with huge storage, we choose to extract multiple modalities (including i-frames, motion vectors, and residuals) directly from compressed videos. by employing mobilenetv2 as backbone, we propose a novel temporal trilinear pooling (ttp) module to fuse the multiple modalities for mobile video action recognition. in addition to motion vectors, we also provide a temporal fusion method to explicitly induce the temporal context. the efficiency test on a mobile device indicates that our model can perform mobile video action recognition at about 40fps. the comparative results on two benchmarks show that our model outperforms existing action recognition methods in model size and time consuming, but with competitive accuracy.
 we show how to build several data structures of central importance to string processing, taking as input the burrows-wheeler transform (bwt) and using small extra working space. let @math be the text length and @math be the alphabet size. we first provide two algorithms that enumerate all lcp values and suffix tree intervals in @math time using just @math bits of working space on top of the input bwt. using these algorithms as building blocks, for any parameter @math we show how to build the plcp bitvector and the balanced parentheses representation of the suffix tree topology in @math time using at most @math bits of working space on top of the input bwt and the output. in particular, this implies that we can build a compressed suffix tree from the bwt using just succinct working space (i.e. @math bits) and any time in @math . this improves the previous most space-efficient algorithms, which worked in @math bits and @math time. we also consider the problem of merging bwts of string collections, and provide a solution running in @math time and using just @math bits of working space. an efficient implementation of our lcp construction and bwt merge algorithms use (in ram) as few as @math bits on top of a packed representation of the input output and process data as fast as @math megabases per second.



Rouge score with annotator1 : 
[{'rouge-1': {'f': 0.07328902374985387, 'p': 0.8143712574850299, 'r': 0.03837110881218847}, 'rouge-2': {'f': 0.03180021473671899, 'p': 0.354, 'r': 0.01664785553047404}, 'rouge-l': {'f': 0.11219195697849957, 'p': 0.6784313725490196, 'r': 0.06115235065394132}}]


Rouge score with annotator2 : 
[{'rouge-1': {'f': 0.07328902374985387, 'p': 0.8143712574850299, 'r': 0.03837110881218847}, 'rouge-2': {'f': 0.03180021473671899, 'p': 0.354, 'r': 0.01664785553047404}, 'rouge-l': {'f': 0.11219195697849957, 'p': 0.6784313725490196, 'r': 0.06115235065394132}}]


Rouge score with annotator3 : 
[{'rouge-1': {'f': 0.07328902374985387, 'p': 0.8143712574850299, 'r': 0.03837110881218847}, 'rouge-2': {'f': 0.03180021473671899, 'p': 0.354, 'r': 0.01664785553047404}, 'rouge-l': {'f': 0.11219195697849957, 'p': 0.6784313725490196, 'r': 0.06115235065394132}}]


average Rouge score : 
{'rouge-l': {'f': 0.11219195697849955, 'r': 0.06115235065394132, 'p': 0.6784313725490195}, 'rouge-2': {'f': 0.03180021473671899, 'r': 0.01664785553047404, 'p': 0.3539999999999999}, 'rouge-1': {'f': 0.07328902374985387, 'r': 0.03837110881218847, 'p': 0.8143712574850298}}


anti-redundance objective value :	nan
Average tf-idf objective value :	7.298344