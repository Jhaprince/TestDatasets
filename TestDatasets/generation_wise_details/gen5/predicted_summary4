 bert (, 2018) and roberta (, 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (sts). however, it requires that both sentences are fed into the network, which causes a massive computational overhead: finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ( 65 hours) with bert. the construction of bert makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. in this publication, we present sentence-bert (sbert), a modification of the pretrained bert network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. this reduces the effort for finding the most similar pair from 65 hours with bert roberta to about 5 seconds with sbert, while maintaining the accuracy from bert. we evaluate sbert and sroberta on common sts tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.
 video action recognition, which is topical in computer vision and video analysis, aims to allocate a short video clip to a pre-defined category such as brushing hair or climbing stairs. recent works focus on action recognition with deep neural networks that achieve state-of-the-art results in need of high-performance platforms. despite the fast development of mobile computing, video action recognition on mobile devices has not been fully discussed. in this paper, we focus on the novel mobile video action recognition task, where only the computational capabilities of mobile devices are accessible. instead of raw videos with huge storage, we choose to extract multiple modalities (including i-frames, motion vectors, and residuals) directly from compressed videos. by employing mobilenetv2 as backbone, we propose a novel temporal trilinear pooling (ttp) module to fuse the multiple modalities for mobile video action recognition. in addition to motion vectors, we also provide a temporal fusion method to explicitly induce the temporal context. the efficiency test on a mobile device indicates that our model can perform mobile video action recognition at about 40fps. the comparative results on two benchmarks show that our model outperforms existing action recognition methods in model size and time consuming, but with competitive accuracy.
 we address the problem of cross-modal fine-grained action retrieval between text and video. cross-modal retrieval is commonly achieved through learning a shared embedding space, that can indifferently embed modalities. in this paper, we propose to enrich the embedding by disentangling parts-of-speech (pos) in the accompanying captions. we build a separate multi-modal embedding space for each pos tag. the outputs of multiple pos embeddings are then used as input to an integrated multi-modal space, where we perform action retrieval. all embeddings are trained jointly through a combination of pos-aware and pos-agnostic losses. our proposal enables learning specialised embedding spaces that offer multiple views of the same embedded entities. we report the first retrieval results on fine-grained actions for the large-scale epic dataset, in a generalised zero-shot setting. results show the advantage of our approach for both video-to-text and text-to-video action retrieval. we also demonstrate the benefit of disentangling the pos for the generic task of cross-modal video retrieval on the msr-vtt dataset.



Rouge score with annotator1 : 
[{'rouge-1': {'f': 0.0742134973570139, 'p': 0.7900763358778626, 'r': 0.038935389824132416}, 'rouge-2': {'f': 0.023307932768302595, 'p': 0.248565965583174, 'r': 0.012227238525206923}, 'rouge-l': {'f': 0.12154340671633049, 'p': 0.6725978647686833, 'r': 0.06680805938494168}}]


Rouge score with annotator2 : 
[{'rouge-1': {'f': 0.0742134973570139, 'p': 0.7900763358778626, 'r': 0.038935389824132416}, 'rouge-2': {'f': 0.023307932768302595, 'p': 0.248565965583174, 'r': 0.012227238525206923}, 'rouge-l': {'f': 0.12154340671633049, 'p': 0.6725978647686833, 'r': 0.06680805938494168}}]


Rouge score with annotator3 : 
[{'rouge-1': {'f': 0.0742134973570139, 'p': 0.7900763358778626, 'r': 0.038935389824132416}, 'rouge-2': {'f': 0.023307932768302595, 'p': 0.248565965583174, 'r': 0.012227238525206923}, 'rouge-l': {'f': 0.12154340671633049, 'p': 0.6725978647686833, 'r': 0.06680805938494168}}]


average Rouge score : 
{'rouge-l': {'f': 0.1215434067163305, 'r': 0.06680805938494168, 'p': 0.6725978647686833}, 'rouge-2': {'f': 0.023307932768302595, 'r': 0.012227238525206923, 'p': 0.248565965583174}, 'rouge-1': {'f': 0.0742134973570139, 'r': 0.038935389824132416, 'p': 0.7900763358778627}}


anti-redundance objective value :	nan
Average tf-idf objective value :	8.130261333333332