 we address the problem of cross-modal fine-grained action retrieval between text and video. cross-modal retrieval is commonly achieved through learning a shared embedding space, that can indifferently embed modalities. in this paper, we propose to enrich the embedding by disentangling parts-of-speech (pos) in the accompanying captions. we build a separate multi-modal embedding space for each pos tag. the outputs of multiple pos embeddings are then used as input to an integrated multi-modal space, where we perform action retrieval. all embeddings are trained jointly through a combination of pos-aware and pos-agnostic losses. our proposal enables learning specialised embedding spaces that offer multiple views of the same embedded entities. we report the first retrieval results on fine-grained actions for the large-scale epic dataset, in a generalised zero-shot setting. results show the advantage of our approach for both video-to-text and text-to-video action retrieval. we also demonstrate the benefit of disentangling the pos for the generic task of cross-modal video retrieval on the msr-vtt dataset.
 we study the simulation of stellar mergers, which requires complex simulations with high computational demands. we have developed octo-tiger, a finite volume grid-based hydrodynamics simulation code with adaptive mesh refinement which is unique in conserving both linear and angular momentum to machine precision. to face the challenge of increasingly complex, diverse, and heterogeneous hpc systems, octo-tiger relies on high-level programming abstractions. we use hpx with its futurization capabilities to ensure scalability both between nodes and within, and present first results replacing mpi with libfabric achieving up to a 2.8x speedup. we extend octo-tiger to heterogeneous gpu-accelerated supercomputers, demonstrating node-level performance and portability. we show scalability up to full system runs on piz daint. for the scenario's maximum resolution, the compute-critical parts (hydrodynamics and gravity) achieve 68.1 parallel efficiency at 2048 nodes.
 spurred by the potential of deep learning, computational music generation has gained renewed academic interest. a crucial issue in music generation is that of user control, especially in scenarios where the music generation process is conditioned on existing musical material. here we propose a model for conditional kick drum track generation that takes existing musical material as input, in addition to a low-dimensional code that encodes the desired relation between the existing material and the new material to be generated. these relational codes are learned in an unsupervised manner from a music dataset. we show that codes can be sampled to create a variety of musically plausible kick drum tracks and that the model can be used to transfer kick drum patterns from one song to another. lastly, we demonstrate that the learned codes are largely invariant to tempo and time-shift.



Rouge score with annotator1 : 
[{'rouge-1': {'f': 0.059625981716819466, 'p': 0.7568807339449541, 'r': 0.031035455656917146}, 'rouge-2': {'f': 0.018794613864820002, 'p': 0.23908045977011494, 'r': 0.009781790820165538}, 'rouge-l': {'f': 0.10136452093010577, 'p': 0.6265060240963856, 'r': 0.05514316012725345}}]


Rouge score with annotator2 : 
[{'rouge-1': {'f': 0.059625981716819466, 'p': 0.7568807339449541, 'r': 0.031035455656917146}, 'rouge-2': {'f': 0.018794613864820002, 'p': 0.23908045977011494, 'r': 0.009781790820165538}, 'rouge-l': {'f': 0.10136452093010577, 'p': 0.6265060240963856, 'r': 0.05514316012725345}}]


Rouge score with annotator3 : 
[{'rouge-1': {'f': 0.059625981716819466, 'p': 0.7568807339449541, 'r': 0.031035455656917146}, 'rouge-2': {'f': 0.018794613864820002, 'p': 0.23908045977011494, 'r': 0.009781790820165538}, 'rouge-l': {'f': 0.10136452093010577, 'p': 0.6265060240963856, 'r': 0.05514316012725345}}]


average Rouge score : 
{'rouge-l': {'f': 0.10136452093010577, 'r': 0.05514316012725345, 'p': 0.6265060240963856}, 'rouge-2': {'f': 0.018794613864820002, 'r': 0.009781790820165538, 'p': 0.23908045977011494}, 'rouge-1': {'f': 0.059625981716819466, 'r': 0.031035455656917146, 'p': 0.756880733944954}}


anti-redundance objective value :	nan
Average tf-idf objective value :	8.121217666666666