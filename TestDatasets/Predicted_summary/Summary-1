 this paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. the methods described in this paper, mcquitty's similarity analysis, ward's minimum-variance method, and the em algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. these methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. overall, the most accurate of these procedures is mcquitty's similarity analysis in combination with a high dimensional feature set.
 in this paper, we study a family of non-convex and possibly non-smooth inf-projection minimization problems, where the target objective function is equal to minimization of a joint function over another variable. this problem includes difference of convex (dc) functions and a family of bi-convex functions as special cases. we develop stochastic algorithms and establish their first-order convergence for finding a (nearly) stationary solution of the target non-convex function under different conditions of the component functions. to the best of our knowledge, this is the first work that comprehensively studies stochastic optimization of non-convex inf-projection minimization problems with provable convergence guarantee. our algorithms enable efficient stochastic optimization of a family of non-decomposable dc functions and a family of bi-convex functions. to demonstrate the power of the proposed algorithms we consider an important application in variance-based regularization, and experiments verify the effectiveness of our inf-projection based formulation and the proposed stochastic algorithm in comparison with previous stochastic algorithms based on the min-max formulation for achieving the same effect.
 in order to mimic the human ability of continual acquisition and transfer of knowledge across various tasks, a learning system needs the capability for continual learning, effectively utilizing the previously acquired skills. as such, the key challenge is to transfer and generalize the knowledge learned from one task to other tasks, avoiding forgetting and interference of previous knowledge and improving the overall performance. in this paper, within the continual learning paradigm, we introduce a method that effectively forgets the less useful data samples continuously and allows beneficial information to be kept for training of the subsequent tasks, in an online manner. the method uses statistical leverage score information to measure the importance of the data samples in every task and adopts frequent directions approach to enable a continual or life-long learning property. this effectively maintains a constant training size across all tasks. we first provide mathematical intuition for the method and then demonstrate its effectiveness in avoiding catastrophic forgetting and computational efficiency on continual learning of classification tasks when compared with the existing state-of-the-art techniques.


Rouge score with annotator1 : 
[{'rouge-1': {'f': 0.0650171565406573, 'p': 0.8163265306122449, 'r': 0.033856860716636884}, 'rouge-2': {'f': 0.02131502813852161, 'p': 0.2681818181818182, 'r': 0.01109857035364936}, 'rouge-l': {'f': 0.10643160159240347, 'p': 0.6965811965811965, 'r': 0.0576175326970661}}]


Rouge score with annotator2 : 
[{'rouge-1': {'f': 0.0650171565406573, 'p': 0.8163265306122449, 'r': 0.033856860716636884}, 'rouge-2': {'f': 0.02131502813852161, 'p': 0.2681818181818182, 'r': 0.01109857035364936}, 'rouge-l': {'f': 0.10643160159240347, 'p': 0.6965811965811965, 'r': 0.0576175326970661}}]


Rouge score with annotator3 : 
[{'rouge-1': {'f': 0.0650171565406573, 'p': 0.8163265306122449, 'r': 0.033856860716636884}, 'rouge-2': {'f': 0.02131502813852161, 'p': 0.2681818181818182, 'r': 0.01109857035364936}, 'rouge-l': {'f': 0.10643160159240347, 'p': 0.6965811965811965, 'r': 0.0576175326970661}}]
